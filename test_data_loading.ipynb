{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_detection.data import load_wili_2018_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wili_2018_data_path = \"/home/derek/PythonProjects/language_detection/datasets/WiLi_2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-26 12:19:50.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_detection.data.loaders\u001b[0m:\u001b[36mload_wili_2018_dataset\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m'drop_duplicates' is true, dropping duplicates from *training* set...\u001b[0m\n",
      "\u001b[32m2023-11-26 12:19:50.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_detection.data.loaders\u001b[0m:\u001b[36mload_wili_2018_dataset\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mdropped 3117 samples from training data that also appeared in the test data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wiki_dataset = load_wili_2018_dataset(wili_2018_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(wiki_dataset.x_train).intersection(set(wiki_dataset.x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_train': list[str],\n",
       " 'x_test': list[str],\n",
       " 'y_train': list[str],\n",
       " 'y_test': list[str],\n",
       " 'idx2lang': dict[int, str],\n",
       " 'lang2idx': dict[str, int],\n",
       " 'labels': dict[str, str] | None,\n",
       " 'dropped': list[str] | None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.',\n",
       " 'Sebes, Joseph; Pereira Thomas (1961) (på eng). The Jesuits and the Sino-Russian treaty of Nerchinsk (1689): the diary of Thomas Pereira. Bibliotheca Instituti historici S. I., 99-0105377-3 ; 18. Rome. Libris 677492',\n",
       " 'भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षेत्रीय आह्वान, उत्तेजनासभ एवम प्रयत्नसँ प्रेरित, भारतीय राजनैतिक सङ्गठनद्वारा सञ्चालित अहिंसावादी आ सैन्यवादी आन्दोलन छल, जेकर एक समान उद्देश्य, अङ्ग्रेजी शासनक भारतीय उपमहाद्वीपसँ जडीसँ उखाड फेकनाई छल। ई आन्दोलनक शुरुआत १८५७ मे भेल सिपाही विद्रोहक मानल जाइत अछि। स्वाधीनताक लेल हजारो लोग अपन प्राणक बलि देलक। भारतीय राष्ट्रीय कांग्रेस १९३० कांग्रेस अधिवेशन मे अङ्ग्रेजसँ पूर्ण स्वराजक मांग केने छल।']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset.x_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoryview(bytes(\"hello, world\", encoding=\"utf8\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text = wiki_dataset.x_train[2]\n",
    "mask_pct = 0.15\n",
    "\n",
    "# convert to integer sequence of bytes\n",
    "byte_sequence: np.ndarray = np.array(memoryview(bytes(text, encoding=\"utf8\")).tolist())\n",
    "# offset by 4 (for pad=0, cls=1, eos=2, and mask=3)\n",
    "byte_sequence += 4\n",
    "# add cls (1) and eos (2)\n",
    "target_sequence = np.concatenate((np.array([1]), byte_sequence, np.array([2])))\n",
    "input_sequence = np.copy(target_sequence)\n",
    "# BERT mask inputs at mask_pct rate. of those, mask 80%, random value 10%, keep 10%\n",
    "mod_count = int(np.round(len(byte_sequence)*mask_pct))\n",
    "to_mod_idxs = np.random.choice(np.arange(1, len(byte_sequence)-2), replace=False, size=mod_count)\n",
    "split_indices = np.cumsum([int(len(to_mod_idxs)*0.80), int(len(to_mod_idxs)*0.10), int(len(to_mod_idxs)*0.10)])\n",
    "split_indices[-1] = len(to_mod_idxs)\n",
    "mask_idxs, repl_idxs, keep_idxs, _ = np.split(to_mod_idxs, split_indices)\n",
    "target_sequence[mask_idxs] = 2\n",
    "target_sequence[repl_idxs] = np.random.randint(4, 256+4, size=(len(repl_idxs),))\n",
    "modded_idxs = np.sort(to_mod_idxs)\n",
    "\n",
    "# check\n",
    "diff_idx = np.where(np.invert(np.isclose(input_sequence, target_sequence)))\n",
    "okay = np.isin(diff_idx, modded_idxs).all(1)[0]\n",
    "assert okay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text transformation function\n",
    "def transform_text(text: str):\n",
    "    byte_sequence: ArrayLike = np.array(memoryview(bytes(text, encoding=\"utf8\")).tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
