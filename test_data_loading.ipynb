{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_detection.data import load_wili_2018_dataset, BytesDataset, batch_collate_function, get_mask_from_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wili_2018_data_path = \"/home/derek/PythonProjects/language_detection/datasets/WiLi_2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-26 18:20:17.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_detection.data.loaders\u001b[0m:\u001b[36mload_wili_2018_dataset\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m'drop_duplicates' is true, dropping duplicates from *training* set...\u001b[0m\n",
      "\u001b[32m2023-11-26 18:20:17.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_detection.data.loaders\u001b[0m:\u001b[36mload_wili_2018_dataset\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mdropped 3117 samples from training data that also appeared in the test data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wiki_dataset = load_wili_2018_dataset(wili_2018_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(wiki_dataset.x_train).intersection(set(wiki_dataset.x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_train': list[str],\n",
       " 'x_test': list[str],\n",
       " 'y_train': list[str],\n",
       " 'y_test': list[str],\n",
       " 'idx2lang': dict[int, str],\n",
       " 'lang2idx': dict[str, int],\n",
       " 'labels': dict[str, str] | None,\n",
       " 'dropped': list[str] | None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.',\n",
       " 'Sebes, Joseph; Pereira Thomas (1961) (på eng). The Jesuits and the Sino-Russian treaty of Nerchinsk (1689): the diary of Thomas Pereira. Bibliotheca Instituti historici S. I., 99-0105377-3 ; 18. Rome. Libris 677492',\n",
       " 'भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षेत्रीय आह्वान, उत्तेजनासभ एवम प्रयत्नसँ प्रेरित, भारतीय राजनैतिक सङ्गठनद्वारा सञ्चालित अहिंसावादी आ सैन्यवादी आन्दोलन छल, जेकर एक समान उद्देश्य, अङ्ग्रेजी शासनक भारतीय उपमहाद्वीपसँ जडीसँ उखाड फेकनाई छल। ई आन्दोलनक शुरुआत १८५७ मे भेल सिपाही विद्रोहक मानल जाइत अछि। स्वाधीनताक लेल हजारो लोग अपन प्राणक बलि देलक। भारतीय राष्ट्रीय कांग्रेस १९३० कांग्रेस अधिवेशन मे अङ्ग्रेजसँ पूर्ण स्वराजक मांग केने छल।']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset.x_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_byte_sequences = [memoryview(bytes(c, encoding=\"utf8\")).tolist() for c in wiki_dataset.x_train[2]]\n",
    "char_seq_lens = [len(c) for c in char_byte_sequences]\n",
    "if sum(char_seq_lens) > max_length:\n",
    "    for idx in reversed(range(len(char_seq_lens))):\n",
    "        subtotal = sum(char_seq_lens[:idx])\n",
    "        if subtotal <= max_length -2:\n",
    "            char_byte_sequences = char_byte_sequences[:idx]\n",
    "            break\n",
    "byte_sequence = [byte_val for char in char_byte_sequences for byte_val in char]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        joint classification and masked language model transformer encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = 256+4\n",
    "        self.num_classes = 235\n",
    "        self.transformer_layer_count = 4\n",
    "        self.ffn_dims = 1024\n",
    "        self.output_dims = 512\n",
    "        self.attn_heads = 4\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=self.num_features, embedding_dim=self.output_dims)\n",
    "        self.transformer_layers = torch.nn.ModuleList([\n",
    "            torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dims, \n",
    "                nhead=self.attn_heads, \n",
    "                dim_feedforward=self.ffn_dims, \n",
    "                activation=\"gelu\", \n",
    "                batch_first=True\n",
    "            ) for _ in range(self.transformer_layer_count)\n",
    "        ])\n",
    "        self.clf_layer = torch.nn.Linear(in_features=self.output_dims, out_features=self.num_classes)\n",
    "        self.mlm_layer = torch.nn.Linear(in_features=self.output_dims, out_features=self.num_features)\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        x = self.embedding(x)\n",
    "        for lyr in self.transformer_layers:\n",
    "            x = lyr(x, src_key_padding_mask=pad_mask)\n",
    "        mlm_preds = self.mlm_layer(x)\n",
    "        clf_preds = self.clf_layer(x[:, 0, :])\n",
    "        return clf_preds, mlm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all unmentioned params on cuda!\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier()\n",
    "_ = model.to(\"cuda\")\n",
    "for name, param in model.named_parameters():\n",
    "    if not str(param.device).startswith(\"cuda\"):\n",
    "        print(f\"param '{name}' is on device '{param.device}'\")\n",
    "print(f\"all unmentioned params on cuda!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 102944, dev: 11439, test: 117500\n"
     ]
    }
   ],
   "source": [
    "from zmq import device\n",
    "\n",
    "train_dev_split = int(len(wiki_dataset.x_train) * 0.9)\n",
    "\n",
    "train_dataset = BytesDataset(\n",
    "    texts=wiki_dataset.x_train[:train_dev_split], \n",
    "    languages=wiki_dataset.y_train[:train_dev_split], \n",
    "    mapping=wiki_dataset.lang2idx, \n",
    "    max_length=1024, \n",
    "    is_training=True\n",
    ")\n",
    "dev_dataset = BytesDataset(\n",
    "    texts=wiki_dataset.x_train[train_dev_split:], \n",
    "    languages=wiki_dataset.y_train[train_dev_split:], \n",
    "    mapping=wiki_dataset.lang2idx, \n",
    "    max_length=1024, \n",
    "    is_training=True\n",
    ")\n",
    "test_dataset = BytesDataset(\n",
    "    texts=wiki_dataset.y_test, \n",
    "    languages=wiki_dataset.y_test, \n",
    "    mapping=wiki_dataset.lang2idx, \n",
    "    max_length=1024, \n",
    "    is_training=False\n",
    ")\n",
    "print(f\"train: {len(train_dataset)}, dev: {len(dev_dataset)}, test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=batch_collate_function)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=batch_collate_function)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=batch_collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 2: clf: 90.068, mlm: 6734.601:   0%|          | 1/6434 [00:00<1:15:46,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024]) torch.Size([16, 1024]) torch.Size([16, 1024]) torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 244: clf: 70.579, mlm: 3189.291:   4%|▍         | 244/6434 [00:59<25:12,  4.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/derek/PythonProjects/language_detection/test_data_loading.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Birene/home/derek/PythonProjects/language_detection/test_data_loading.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, minibatch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar \u001b[39m:=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(train_iterator, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader))):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Birene/home/derek/PythonProjects/language_detection/test_data_loading.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# format data and move to gpu\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Birene/home/derek/PythonProjects/language_detection/test_data_loading.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     x, y, seq_lens, mask_indices, targets \u001b[39m=\u001b[39m minibatch\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Birene/home/derek/PythonProjects/language_detection/test_data_loading.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Birene/home/derek/PythonProjects/language_detection/test_data_loading.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Birene/home/derek/PythonProjects/language_detection/test_data_loading.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "accumulate_steps = 4\n",
    "global_steps = 0\n",
    "\n",
    "mlm_criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=-1)\n",
    "clf_criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=0.001, \n",
    "    epochs=total_epochs,\n",
    "    steps_per_epoch=len(train_dataloader)//accumulate_steps, \n",
    "    pct_start=0.1\n",
    ")\n",
    "for epoch in range(total_epochs):\n",
    "    train_iterator = iter(train_dataloader)\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    print(f\"epoch {epoch+1} training\")\n",
    "    time.sleep(0.5)\n",
    "    for batch_idx, minibatch in enumerate(pbar := tqdm.tqdm(train_iterator, total=len(train_dataloader))):\n",
    "        # format data and move to gpu\n",
    "        x, y, seq_lens, mask_indices, targets = minibatch\n",
    "        x = x.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        targets = targets.to(\"cuda\")\n",
    "        pad_mask = get_mask_from_lengths(seq_lens, 1024, x.device)\n",
    "        # model forward\n",
    "        clf_logits, mlm_logits = model.forward(x, pad_mask)\n",
    "        # calculate losses\n",
    "        masked_y = -1 * torch.ones_like(y).to(\"cuda\")\n",
    "        for i in range(y.shape[0]):\n",
    "            masked_y[i, mask_indices[i].long()] = y[i, mask_indices[i].long()]\n",
    "        mlm_loss = mlm_criterion(torch.transpose(mlm_logits, 1, 2), masked_y)\n",
    "        clf_loss = clf_criterion(clf_logits, targets)\n",
    "        global_steps += 1\n",
    "        # windowed loss display\n",
    "        epoch_losses.append((clf_loss.item(), mlm_loss.item()))\n",
    "        clf_losses = np.mean([l[0] for l in epoch_losses][-5:])\n",
    "        mlm_losses = np.mean([l[1] for l in epoch_losses][-5:])\n",
    "        pbar.set_description(f\"step {global_steps}: clf: {clf_losses:.3f}, mlm: {mlm_losses:.3f}\")\n",
    "        # gradient accumulation\n",
    "        mlm_loss /= accumulate_steps\n",
    "        clf_loss /= accumulate_steps\n",
    "        ttl_loss = mlm_loss + clf_loss\n",
    "        ttl_loss.backward()\n",
    "        if (batch_idx > 0 and batch_idx % accumulate_steps == 0) or (batch_idx == len(train_dataloader)):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    # dev eval\n",
    "    print(f\"epoch {epoch+1} dev eval\")\n",
    "    time.sleep(0.5)\n",
    "    model.eval()\n",
    "    epoch_dev_loss = []\n",
    "    epoch_targets = []\n",
    "    epoch_predictions = []\n",
    "    with torch.no_grad():\n",
    "        dev_iterator = iter(dev_dataloader)\n",
    "        for batch_idx, minibatch in enumerate(pbar := tqdm.tqdm(dev_iterator, total=len(dev_iterator))):\n",
    "            # format data and move to gpu\n",
    "            x, y, seq_lens, mask_indices, targets = minibatch\n",
    "            x = x.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            targets = targets.to(\"cuda\")\n",
    "            pad_mask = get_mask_from_lengths(seq_lens, 1024, x.device)\n",
    "            # model forward\n",
    "            clf_logits, mlm_logits = model.forward(x, pad_mask)\n",
    "            # calculate losses\n",
    "            masked_y = -1 * torch.ones_like(y).to(\"cuda\")\n",
    "            for i in range(y.shape[0]):\n",
    "                masked_y[i, mask_indices[i].long()] = y[i, mask_indices[i].long()]\n",
    "            mlm_loss = mlm_criterion(torch.transpose(mlm_logits, 1, 2), masked_y)\n",
    "            clf_loss = clf_criterion(clf_logits, targets)\n",
    "            epoch_dev_loss.append(clf_loss.item())\n",
    "            epoch_targets += targets.detach().cpu().numpy().tolist()\n",
    "            epoch_predictions += clf_logits.max(1).indices.detach().cpu().numpy().tolist()\n",
    "        print(f\"dev micro prc: {precision_score(epoch_targets, epoch_predictions, average='micro')}\")\n",
    "        print(f\"dev micro rcl: {recall_score(epoch_targets, epoch_predictions, average='micro')}\")\n",
    "        print(f\"dev micro f1b: {f1_score(epoch_targets, epoch_predictions, average='micro')}\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
